{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73e99c0b",
   "metadata": {},
   "source": [
    "# Preprocess the MESA Dataset\n",
    "\n",
    "The MESA dataset is an extensive collection of sleep-related data. This notebook is designed to process the downloaded MESA dataset, extract the required information, and store it in a processed format for easier analysis.\n",
    "\n",
    "## Source of Dataset\n",
    "You can obtain the MESA dataset from [sleepdata.org](https://sleepdata.org/datasets/mesa). Please note that approval is required before you can download the data.\n",
    "\n",
    "## Data Structure and Processing Steps\n",
    "\n",
    "### 1. Data Locations:\n",
    "- **EDF Files (for HR)**: `data/mesa/polysomnography/edfs/`\n",
    "- **Activity CSV Files**: `data/mesa/actigraphy/`\n",
    "- **Sleep Scoring XML Files**: `data/mesa/polysomnography/annotations-events-nsrr/`\n",
    "\n",
    "### 2. Processed Data Storage:\n",
    "The processed data, along with the relevant timestamps, will be stored in the `data/mesa/processed/` directory. Three files will be generated for each subject.\n",
    "\n",
    "### 3. Naming Convention for Processed Files:\n",
    "- **Activity Count**: `activity_min_len-{fileid}.csv`\n",
    "- **HR**: `hr_min_len-{fileid}.csv`\n",
    "- **Sleep Scoring**: `sleep_min_len-{fileid}.csv`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2419123",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd71fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants used throughout the data processing workflow\n",
    "\n",
    "EDF_FOLDER_PATH = 'data/mesa/polysomnography/edfs/'\n",
    "PSG_FOLDER_PATH = 'data/mesa/polysomnography/annotations-events-nsrr/'\n",
    "ACTIVITY_FOLDER_PATH = 'data/mesa/actigraphy/'\n",
    "OVERLAY_FILE_PATH = 'data/mesa/overlap/mesa-actigraphy-psg-overlap.csv'\n",
    "EDF_FILE_PATTERN = 'mesa-sleep-{}.edf'\n",
    "PSG_FILE_PATTERN = 'mesa-sleep-{}-nsrr.xml'\n",
    "ACTIVITY_FILE_PATTERN = 'mesa-sleep-{}.csv'\n",
    "EPOCH_DURATION_IN_SECONDS = 30\n",
    "PROCESSED_FOLDER_PATH_PREFIX = 'data/mesa/processed/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0baa16f",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064edaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Utility functions assist in various tasks such as:\n",
    "# - Extracting numeric IDs from filenames.\n",
    "# - Finding common IDs across multiple directories.\n",
    "import csv\n",
    "import numpy as np\n",
    "import logging\n",
    "import re\n",
    "import os\n",
    "import pyedflib\n",
    "\n",
    "\n",
    "# Setting up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Constants for clarity\n",
    "OFF_WRIST = '1'\n",
    "EXCLUDED = 'excluded'\n",
    "\n",
    "def read_activity(actigraphy_file_path, file_id):\n",
    "    logging.info(f\"Trying to read file {actigraphy_file_path}\")\n",
    "    overlap_csv_path = OVERLAY_FILE_PATH\n",
    "\n",
    "    line_align = find_line_alignment(file_id, overlap_csv_path)\n",
    "\n",
    "    if line_align == -1:\n",
    "        logging.warning(f\"Line alignment not found for file id {file_id}\")\n",
    "    else:\n",
    "        logging.info(f\"Line alignment found for file id {file_id} and number is {line_align}\")\n",
    "\n",
    "    activity_data = extract_activity(line_align, actigraphy_file_path)\n",
    "    activity_data = clean_data(activity_data)\n",
    "    return activity_data\n",
    "\n",
    "def read_csv_file(file_path, start_from_second_row=True):\n",
    "    with open(file_path, 'r') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        if start_from_second_row:\n",
    "            next(csv_reader)\n",
    "        return list(csv_reader)\n",
    "\n",
    "def find_line_alignment(file_id, overlap_csv_path):\n",
    "    rows = read_csv_file(overlap_csv_path)\n",
    "    for row in rows:\n",
    "        if int(row[0]) == int(file_id):\n",
    "            return int(row[1])\n",
    "    return -1\n",
    "\n",
    "def extract_activity(line_align, actigraphy_csv_path):\n",
    "    rows = read_csv_file(actigraphy_csv_path)\n",
    "    elapsed_time_counter = 0\n",
    "    activity = []\n",
    "\n",
    "    for row in rows:\n",
    "        if row[3] == OFF_WRIST or row[11] == EXCLUDED:\n",
    "            continue\n",
    "        if int(row[1]) >= line_align:\n",
    "            activity_value = np.nan if row[4] == '' else float(row[4])\n",
    "            activity.append([elapsed_time_counter, activity_value])\n",
    "            elapsed_time_counter += EPOCH_DURATION_IN_SECONDS\n",
    "\n",
    "    activity_array = np.array(activity)\n",
    "    if activity_array.size == 0:\n",
    "        logging.warning(f\"Warning: Empty activity data for file: {actigraphy_csv_path}\")\n",
    "        return np.array([[0, np.nan]])\n",
    "    return activity_array\n",
    "\n",
    "def clean_data(array):\n",
    "    # Only remove rows with NaN values in the second column\n",
    "    valid_rows = ~np.isnan(array[:, 1])\n",
    "    array = array[valid_rows]\n",
    "    \n",
    "    # Remove rows containing infinity values (if any)\n",
    "    array = array[~np.isinf(array).any(axis=1)]\n",
    "    return array\n",
    "\n",
    "def get_common_ids(folder_paths):\n",
    "        \"\"\"\n",
    "        Finds the numeric IDs that are common across all given folders.\n",
    "        \n",
    "        Parameters:\n",
    "        - folder_paths (list): A list of folder paths to compare.\n",
    "        \n",
    "        Returns:\n",
    "        - common_ids (set): A set of numeric IDs that are common to all folders.\n",
    "        \"\"\"\n",
    "        common_ids = None\n",
    "        \n",
    "        for folder in folder_paths:\n",
    "            # List all files in the folder\n",
    "            filenames = os.listdir(folder)\n",
    "            \n",
    "            # Extract numeric IDs from the filenames\n",
    "            ids_in_current_folder = {get_numeric_id(filename) for filename in filenames}\n",
    "            \n",
    "            if common_ids is None:\n",
    "                # For the first folder, set the common IDs set as the IDs in this folder\n",
    "                common_ids = ids_in_current_folder\n",
    "            else:\n",
    "                # Find the intersection with the IDs from this folder\n",
    "                common_ids = common_ids.intersection(ids_in_current_folder)\n",
    "                \n",
    "        return common_ids\n",
    "\n",
    "def get_numeric_id(filename):\n",
    "        \"\"\"\n",
    "        Extracts the numeric ID from a filename using regular expressions.\n",
    "        \"\"\"\n",
    "        match = re.search(r'\\d+', filename)\n",
    "        return match.group(0) if match else None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39f6145",
   "metadata": {},
   "source": [
    "## Main Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e608997",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Main functions for processing different aspects of the MESA dataset:\n",
    "# - HR Data Processing\n",
    "# - PSG Data Processing\n",
    "# - Activity Data Processing\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "# Setting up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def process_subjects(common_ids):\n",
    "    for file_id in common_ids:\n",
    "        # Check if files exist\n",
    "        if files_already_processed(file_id):\n",
    "            logging.info(f\"Files for file id {file_id} already processed. Skipping...\")\n",
    "            continue\n",
    "        # HR Data\n",
    "        hr_data = read_hr(os.path.join(EDF_FOLDER_PATH, EDF_FILE_PATTERN.format(file_id)))\n",
    "        # PSG Data\n",
    "        psg_data = read_psg(os.path.join(PSG_FOLDER_PATH, PSG_FILE_PATTERN.format(file_id)),\n",
    "                                     EPOCH_DURATION_IN_SECONDS)\n",
    "        # Activity Data\n",
    "        activity_data = read_activity(os.path.join(ACTIVITY_FOLDER_PATH, ACTIVITY_FILE_PATTERN.format(file_id)),\n",
    "                                                    file_id)\n",
    "        # Generate output files\n",
    "        output_folder_prefix = PROCESSED_FOLDER_PATH_PREFIX\n",
    "        write_data_with_min_len(file_id, activity_data, hr_data, psg_data, output_folder_prefix)\n",
    "\n",
    "def read_hr(file_path):\n",
    "    \"\"\"\n",
    "    Reads heart rate data from an EDF file.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): The full path to the EDF file.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A 2D array where the first column is time in seconds, \n",
    "        and the second column is the heart rate data, or None if there was an error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with pyedflib.EdfReader(file_path) as edf_file:\n",
    "            signal_labels = edf_file.getSignalLabels()\n",
    "            if 'HR' not in signal_labels:\n",
    "                logging.warning(\"HR signal not found in this file.\")\n",
    "                return None\n",
    "\n",
    "            hr_index = signal_labels.index('HR')\n",
    "            sample_frequencies = edf_file.getSampleFrequencies()\n",
    "            hr = edf_file.readSignal(hr_index)\n",
    "            fs = sample_frequencies[hr_index]\n",
    "\n",
    "        time_hr = np.arange(len(hr)) / fs\n",
    "        data = np.column_stack((time_hr, hr))\n",
    "        data = data[data[:, 1] != 0]\n",
    "        \n",
    "        return data\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"File not found: {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred while processing the file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def read_psg(file_path, epoch_duration_in_seconds):\n",
    "    logging.info(f\"Reading PSG data from: {file_path}\")\n",
    "    xml_root = parse_xml_file(file_path)\n",
    "    if xml_root is None:\n",
    "        logging.error(f\"Failed to parse XML from: {file_path}\")\n",
    "        return []\n",
    "\n",
    "    scored_events = extract_scored_events(xml_root)\n",
    "    stages = process_staged_windows(scored_events, epoch_duration_in_seconds)\n",
    "    return stages\n",
    "\n",
    "def files_already_processed(file_id):\n",
    "    activity_output_path = get_output_path('activity', file_id)\n",
    "    hr_output_path = get_output_path('hr', file_id)\n",
    "    sleep_output_path = get_output_path('sleep', file_id)\n",
    "    return os.path.exists(activity_output_path) and os.path.exists(hr_output_path) and os.path.exists(sleep_output_path)\n",
    "\n",
    "def get_output_path(data_type, file_id):\n",
    "    return os.path.join(PROCESSED_FOLDER_PATH_PREFIX, f'{data_type}_min_len_{file_id}.csv')\n",
    "\n",
    "def determine_min_len(activity_data, hr_data, sleep_data):\n",
    "    hr_data_len_30s_equiv = hr_data // 30  # Convert HR data length to its 30-second equivalent\n",
    "    return int(min(activity_data, hr_data_len_30s_equiv, sleep_data))\n",
    "\n",
    "def write_data_with_min_len(file_id, activity_data, hr_data, sleep_data, output_folder_prefix):\n",
    "    # Create the directory if it doesn't exist, relative to the current working directory\n",
    "    full_path = os.path.join(os.getcwd(), PROCESSED_FOLDER_PATH_PREFIX)\n",
    "    if not os.path.exists(full_path):\n",
    "        os.makedirs(full_path)  \n",
    "    # Check if the activity_data only contains NaNs and skip if it does\n",
    "    if np.isnan(activity_data).all():\n",
    "        logging.warning(f\"No valid activity data detected for file id: {file_id}. Skipping...\")\n",
    "        return\n",
    "    min_len = determine_min_len(activity_data[-1][0], hr_data[-1][0], sleep_data[-1][0])\n",
    "    hr_min_len = min_len * 30  # Adjusting for the HR data since it's in seconds\n",
    "    _write_to_csv(activity_data[:min_len],\n",
    "                  os.path.join(output_folder_prefix, f'activity_min_len_{file_id}.csv'),\n",
    "                  ['Elapsed Time', 'Activity Value'])\n",
    "    _write_to_csv(hr_data[:hr_min_len],\n",
    "                  os.path.join(output_folder_prefix, f'hr_min_len_{file_id}.csv'),\n",
    "                  ['Time (s)', 'Heart Rate'])\n",
    "    _write_to_csv(sleep_data[:min_len],\n",
    "                  os.path.join(output_folder_prefix, f'sleep_min_len_{file_id}.csv'),\n",
    "                  ['Timestamp', 'Stage Value'])\n",
    "\n",
    "def _write_to_csv(data, file_path, headers):\n",
    "    with open(file_path, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(headers)\n",
    "        writer.writerows(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b28a16",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f2cf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Main execution flow to process the MESA data\n",
    "import xml.etree.ElementTree as ET\n",
    "import logging\n",
    "\n",
    "# Setting up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Constants for stage mapping\n",
    "STAGE_TO_NUM = {'Wake': 0, 'REM sleep': 5, 'Stage 1 sleep': 1, 'Stage 2 sleep': 2, 'Stage 3 sleep': 3, 'Stage 4 sleep': 4}  \n",
    "\n",
    "def read_psg(file_path, epoch_duration_in_seconds):\n",
    "    logging.info(f\"Reading PSG data from: {file_path}\")\n",
    "    xml_root = parse_xml_file(file_path)\n",
    "    if xml_root is None:\n",
    "        logging.error(f\"Failed to parse XML from: {file_path}\")\n",
    "        return []\n",
    "\n",
    "    scored_events = extract_scored_events(xml_root)\n",
    "    stages = process_staged_windows(scored_events, epoch_duration_in_seconds)\n",
    "    return stages\n",
    "\n",
    "def parse_xml_file(file_path):\n",
    "    try:\n",
    "        tree = ET.parse(file_path)\n",
    "        return tree.getroot()\n",
    "    except ET.ParseError as e:\n",
    "        logging.error(f\"Error parsing XML file {file_path}: {e}\")\n",
    "        return None\n",
    "    \n",
    "def extract_scored_events(root_element):\n",
    "    stage_data = []\n",
    "    for scored_event in root_element.findall('.//ScoredEvent'):\n",
    "        duration = float(scored_event.find('Duration').text)\n",
    "        start = float(scored_event.find('Start').text)\n",
    "        stage = scored_event.find('EventConcept').text.split('|')[0]\n",
    "        \n",
    "        if stage in STAGE_TO_NUM:\n",
    "            stage_data.append([STAGE_TO_NUM[stage], start, duration])\n",
    "    return stage_data\n",
    "    \n",
    "def process_staged_windows(stage_data, epoch_duration_in_seconds):\n",
    "    stages = []\n",
    "    for staged_window in stage_data:\n",
    "        elapsed_time_counter = 0\n",
    "        stage_value = staged_window[0]\n",
    "        duration = staged_window[2]\n",
    "        start = staged_window[1]\n",
    "\n",
    "        while elapsed_time_counter < duration:\n",
    "            timestamp = start + elapsed_time_counter\n",
    "            stages.append([timestamp, stage_value])\n",
    "            elapsed_time_counter += epoch_duration_in_seconds\n",
    "    return stages\n",
    "\n",
    "def process_mesa_data():\n",
    "    folder_paths = [EDF_FOLDER_PATH, PSG_FOLDER_PATH, ACTIVITY_FOLDER_PATH]\n",
    "    common_ids = get_common_ids(folder_paths)\n",
    "    sorted_common_ids = sorted(common_ids, key=int)\n",
    "    process_subjects(sorted_common_ids)    \n",
    "    print(f\"Total number of subjects: {len(sorted_common_ids)}\")\n",
    "\n",
    "\n",
    "process_mesa_data()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
