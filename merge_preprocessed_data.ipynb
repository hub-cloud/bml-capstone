{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81135d33",
   "metadata": {},
   "source": [
    "# Merge PreProcessed MESA Data\n",
    "\n",
    "The notebook `preprocess_data.ipynb` is responsible for generating the data files that are currently housed within the `data/mesa/processed/` directory.\n",
    "These files will now be processed and consolidated.\n",
    "\n",
    "## Dataset Characteristics:\n",
    "\n",
    "- **HR Data**: This data is logged every second.\n",
    "- **Activity and Sleep Data**: Entries for these datasets are made every 30 seconds.\n",
    "\n",
    "## Procedure:\n",
    "\n",
    "1. **Resampling HR Data**: The HR data will be resampled to align with the 30-second interval used by the activity and sleep datasets.\n",
    "2. **Timestamp Comparison**: Timestamps across different data files will be compared to ensure synchronization.\n",
    "3. **Merging Data**: For each subject, data across different files will be merged based on matching timestamps.\n",
    "4. **Final Data Output**: The aggregated data for all subjects will be consolidated and saved to `final_merged_dataset.csv`.\n",
    "5. **Data Anonymization**: All personally identifying information, including subject ID and timestamps, will be stripped to ensure privacy.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e255f1",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d451f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSED_FOLDER_PATH_PREFIX = 'data/mesa/processed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa38f889",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "import sys\n",
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060a52fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a custom logging handler that directs messages to standard output\n",
    "stdout_handler = logging.StreamHandler(sys.stdout)\n",
    "\n",
    "# Define a logging format\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "stdout_handler.setFormatter(formatter)\n",
    "\n",
    "# Get the root logger\n",
    "root_logger = logging.getLogger()\n",
    "\n",
    "# Clear any existing handlers from the root logger\n",
    "root_logger.handlers = []\n",
    "\n",
    "# Add the custom handler to the root logger\n",
    "root_logger.addHandler(stdout_handler)\n",
    "\n",
    "# Set the logging level to DEBUG for the root logger\n",
    "root_logger.setLevel(logging.DEBUG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe97e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_ids(data_dir):\n",
    "    \"\"\"Return unique file IDs from the directory.\"\"\"\n",
    "    return {file.split('_')[-1].split('.')[0] for file in os.listdir(data_dir) if file.endswith('.csv')}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2666bd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_for_id(data_dir, id_val):\n",
    "    \"\"\"Load datasets for a specific ID.\"\"\"\n",
    "    try:\n",
    "        activity_data = pd.read_csv(os.path.join(data_dir, f'activity_min_len_{id_val}.csv'))\n",
    "        hr_data = pd.read_csv(os.path.join(data_dir, f'hr_min_len_{id_val}.csv'))\n",
    "        sleep_data = pd.read_csv(os.path.join(data_dir, f'sleep_min_len_{id_val}.csv'))\n",
    "        return activity_data, hr_data, sleep_data\n",
    "    except FileNotFoundError:\n",
    "        logging.debug(f\"Files for ID: {id_val} not found.\")\n",
    "        return None, None, None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943a0061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_and_merge(activity_data, hr_data, sleep_data):\n",
    "    \"\"\"Resample HR data and merge datasets.\"\"\"\n",
    "    hr_data['Time (s)'] = (hr_data['Time (s)'] // 30) * 30\n",
    "    hr_data_resampled = hr_data.groupby('Time (s)').mean().reset_index()\n",
    "\n",
    "    merged_data = pd.merge(activity_data, hr_data_resampled, left_on='Elapsed Time', right_on='Time (s)', how='inner')\n",
    "    merged_data = pd.merge(merged_data, sleep_data, left_on='Elapsed Time', right_on='Timestamp', how='inner')\n",
    "    \n",
    "    merged_data.drop(columns=['Time (s)', 'Timestamp'], inplace=True)\n",
    "    merged_data.rename(columns={'Elapsed Time': 'Timestamp'}, inplace=True)\n",
    "    return merged_data, hr_data_resampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7215b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_statistics(dataframes, statistics):\n",
    "    \"\"\"Gather statistics for the current ID's data.\"\"\"\n",
    "    data_keys = [\"activity_data\", \"hr_data\", \"sleep_data\", \"hr_data_resampled\", \"merged_data\"]\n",
    "    for key, df in zip(data_keys, dataframes):\n",
    "        if key in [\"activity_data\", \"hr_data\", \"sleep_data\"]:\n",
    "            statistics[\"nan_counts\"][key] += df.isnull().sum().sum()\n",
    "        if key == \"activity_data\":\n",
    "            statistics[\"zero_counts\"][key] += (df['Activity Value'] == 0).sum()\n",
    "        \n",
    "        statistics[\"cumulative_counts\"][key] += len(df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef5fec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_for_id(data_dir, id_val, statistics):\n",
    "    activity_data, hr_data, sleep_data = load_data_for_id(data_dir, id_val)\n",
    "    \n",
    "    if activity_data is not None:\n",
    "        merged_data, hr_data_resampled = resample_and_merge(activity_data, hr_data, sleep_data)\n",
    "        merged_data['file_id'] = id_val\n",
    "        gather_statistics([activity_data, hr_data, sleep_data, hr_data_resampled, merged_data], statistics)\n",
    "        \n",
    "        return merged_data\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a555d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    current_directory = os.getcwd()\n",
    "    data_dir = os.path.join(current_directory, PROCESSED_FOLDER_PATH_PREFIX)\n",
    "\n",
    "    file_ids = get_file_ids(data_dir)\n",
    "    \n",
    "    statistics = {\n",
    "        \"nan_counts\": {\"activity_data\": 0, \"hr_data\": 0, \"sleep_data\": 0},\n",
    "        \"zero_counts\": {\"activity_data\": 0},\n",
    "        \"cumulative_counts\": {\n",
    "            \"activity_data\": 0, \"hr_data\": 0, \"sleep_data\": 0, \n",
    "            \"hr_data_resampled\": 0, \"merged_data\": 0\n",
    "        }\n",
    "    }\n",
    "\n",
    "    merged_data_list = [process_data_for_id(data_dir, id_val, statistics) for id_val in file_ids]\n",
    "    merged_data_list = [data for data in merged_data_list if data is not None]\n",
    "\n",
    "    final_merged_data = pd.concat(merged_data_list, ignore_index=True)\n",
    "    final_merged_data['Stage Value'] = final_merged_data['Stage Value'].apply(lambda x: 0 if x == 0 else 1)\n",
    "    final_merged_data.drop(columns=['file_id', 'Timestamp'], inplace=True)\n",
    "\n",
    "    # Logging statistics\n",
    "    logging.debug(\"\\nNaN Counts in Final Merged Dataset:\")\n",
    "    logging.debug(final_merged_data.isnull().sum())\n",
    "    logging.debug(\"\\nNaN Counts in Original Files:\")\n",
    "    for key, value in statistics[\"nan_counts\"].items():\n",
    "        logging.debug(f\"{key}: {value}\")\n",
    "    logging.debug(\"\\nZero Counts in Activity Data:\")\n",
    "    for key, value in statistics[\"zero_counts\"].items():\n",
    "        logging.debug(f\"{key}: {value}\")\n",
    "    logging.debug(\"\\nCumulative Counts:\")\n",
    "    for key, value in statistics[\"cumulative_counts\"].items():\n",
    "        logging.debug(f\"{key}: {value}\")\n",
    "\n",
    "    final_merged_data.to_csv('final_merged_data.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2384c78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
